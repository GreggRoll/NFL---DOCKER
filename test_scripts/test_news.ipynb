{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(os.getenv('API_KEY'))\n",
    "\n",
    "\n",
    "def get_start_end():\n",
    "    today = datetime.now()\n",
    "    weekday = today.weekday()  # Monday is 0 and Sunday is 6\n",
    "\n",
    "    # Calculate the start date (Tuesday)\n",
    "    if weekday >= 1:  # If today is Tuesday or after\n",
    "        start_date = today - timedelta(days=(weekday - 1))\n",
    "    else:  # If today is before Tuesday\n",
    "        start_date = today - timedelta(days=(weekday + 6))\n",
    "\n",
    "    # Calculate the end date (Monday)\n",
    "    if weekday <= 0:  # If today is Monday\n",
    "        end_date = today\n",
    "    else:  # If today is after Monday\n",
    "        end_date = today + timedelta(days=(7 - weekday))\n",
    "    return start_date, end_date\n",
    "\n",
    "def setup_logger(name):\n",
    "    \"\"\"Set up a logger for a given module.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler('app.log')\n",
    "    formatter = logging.Formatter('%(asctime)s [%(levelname)s] - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # Add the handler to the logger\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "def insert_news(df):\n",
    "    conn = sqlite3.connect('../data-log.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Function to check if a row with the same title exists and if relevant is not None\n",
    "    def row_exists_and_relevant_not_none(title):\n",
    "        cursor.execute(\"SELECT relevant FROM espn_news WHERE title = ?\", (title,))\n",
    "        row = cursor.fetchone()\n",
    "        return row is not None and row[0] is not None\n",
    "\n",
    "    for ix, row in df.iterrows():\n",
    "        if row_exists_and_relevant_not_none(row['title']):\n",
    "            # Update the row\n",
    "            cursor.execute('''\n",
    "            UPDATE espn_news\n",
    "            SET date = ?, link = ?, image_url = ?, relevant = ?, ai_score = ?\n",
    "            WHERE title = ?\n",
    "            ''', (row['date'], row['link'], row['image_url'], row['relevant'], row['ai_score'], row['title']))\n",
    "        else:\n",
    "            # Insert the row if it doesn't exist\n",
    "            cursor.execute('''\n",
    "            INSERT OR IGNORE INTO espn_news (title, date, link, image_url, relevant, ai_score)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (row['title'], row['date'], row['link'], row['image_url'], row['relevant'], row['ai_score']))\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    logger.info(\"INSERTED NEWS TO espn_news\")\n",
    "\n",
    "\n",
    "def get_espn_news():\n",
    "    # URL of the page to scrape\n",
    "    url = \"https://www.nfl.com/news/all-news\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # List to store the scraped data\n",
    "        scraped_data = []\n",
    "\n",
    "        # Function to extract article data\n",
    "        def extract_article_data(article):\n",
    "            # Extract the title\n",
    "            title = article.find('h3', class_='d3-o-media-object__title').get_text(strip=True)\n",
    "\n",
    "            # Extract the date\n",
    "            date = article.find('p', class_='d3-o-media-object__date').get_text(strip=True)\n",
    "\n",
    "            # Extract the link to the article\n",
    "            link = article['href']\n",
    "            full_link = f\"https://www.nfl.com{link}\"\n",
    "\n",
    "            # Extract the image URL\n",
    "            image_tag = article.find('picture').find('img')\n",
    "            image_url = image_tag['src'] if image_tag else None\n",
    "\n",
    "            # Append the extracted data to the list\n",
    "            scraped_data.append({\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': full_link,\n",
    "                'image_url': image_url\n",
    "            })\n",
    "\n",
    "        # Find all vertical article containers\n",
    "        vertical_articles = soup.find_all('div', class_='d3-o-media-object--vertical')\n",
    "        for article in vertical_articles:\n",
    "            extract_article_data(article.find('a'))\n",
    "\n",
    "        # Find all horizontal article containers\n",
    "        horizontal_articles = soup.find_all('a', class_='d3-o-media-object--horizontal')\n",
    "        for article in horizontal_articles:\n",
    "            extract_article_data(article)\n",
    "\n",
    "        df = pd.DataFrame(scraped_data)\n",
    "        df['relevant'] = None\n",
    "        df['ai_score'] = None\n",
    "        insert_news(df)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "def get_unclassified(start_date, end_date):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect('../data-log.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query to get articles within the date range where relevant is None\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM espn_news\n",
    "    WHERE relevant IS NULL\n",
    "    ''')\n",
    "    # 1\n",
    "    # Fetch all results\n",
    "    articles = cursor.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    columns = ['title', 'date', 'link', 'image_url', 'relevant', 'ai_score']\n",
    "    df = pd.DataFrame(articles, columns=columns)\n",
    "    #filter on_days\n",
    "    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n",
    "    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "    return df\n",
    "\n",
    "def update_column(row_name, df):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect('../data-log.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for ix, row in df.iterrows():\n",
    "        cursor.execute(f'''\n",
    "            UPDATE espn_news\n",
    "            SET {row_name} = ?\n",
    "            WHERE title = ?\n",
    "            ''', (row[row_name], row['title']))\n",
    "        \n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    logger.info(f\"UPDATED {row_name} in espn_news\")\n",
    "\n",
    "def extract_article_text(article_url):\n",
    "    article_response = requests.get(article_url)\n",
    "    if article_response.status_code == 200:\n",
    "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "        article_body = article_soup.find('div', class_='nfl-c-article__body')\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all('p')\n",
    "            article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return article_text\n",
    "    return None\n",
    "\n",
    "def check_relevance(title):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Reply True or False to whether the following news title is relevant to the odds of a team winning or losing'\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{title}\"}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def score_article(text):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that reads article texts about sports teams. Your task is to identify the team name mentioned in the article and provide a rating on how the information in the article will affect that team's upcoming game. The rating should be on a scale from 5 to -5, where 5 indicates the team is sure to win and -5 indicates the team is sure to lose.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Example return: [{'Tampa Bay Buccaneers': 3}]\"},\n",
    "        {\"role\": \"system\", \"content\": \"always return json with a list of teams and impacts\"},\n",
    "        {\"role\": \"system\", \"content\": \"always reply in the following format: [{'Team': Score}]\"},\n",
    "        {\"role\": \"system\", \"content\": \"If data is missing or not relevant return [{'None': Score}]\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text}\"}\n",
    "    ],\n",
    "     response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def get_team_ratings(start_date, end_date):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect('../data-log.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query to get articles within the date range where relevant is None\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM espn_news\n",
    "    ''')\n",
    "    # 1\n",
    "    # Fetch all results\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    columns = ['title', 'date', 'link', 'image_url', 'relevant', 'ai_score']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n",
    "    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    df = df[df['ai_score'].notna()]\n",
    "    for ix, row in df.iterrows():\n",
    "        df.at[ix, 'ai_score'] = json.loads(row['ai_score'].strip().replace('\\n', ''))\n",
    "        df_keys = list(row['ai_score'].keys())\n",
    "        if df_keys:\n",
    "            if 'result' in df_keys[0] or 'results' in df_keys[0]:\n",
    "                for game in row['ai_score'][df_keys[0]]:\n",
    "                    final_results.append(game)\n",
    "            else:\n",
    "                final_results.append(row['ai_score'])\n",
    "\n",
    "    def cast_int_or_zero(value):\n",
    "        try:\n",
    "            return(int(value))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    df = pd.DataFrame(final_results)\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df.melt(var_name='TEAM')\n",
    "    # Drop rows with NaN values in the Score column\n",
    "    df_melted['value'] = df_melted['value'].apply(lambda x: cast_int_or_zero(x))\n",
    "    df_melted = df_melted.dropna(subset=['value'])\n",
    "\n",
    "    # Group by TEAM and calculate AVG and SUM\n",
    "    df_grouped = df_melted.groupby('TEAM')['value'].agg(['mean', 'sum']).reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    df_grouped.columns = ['TEAM', 'AVG', 'SUM']\n",
    "\n",
    "    teams = ['Arizona Cardinals',\n",
    "    'Baltimore Ravens',\n",
    "    'Buffalo Bills',\n",
    "    'Chicago Bears',\n",
    "    'Cincinnati Bengals',\n",
    "    'Dallas Cowboys',\n",
    "    'Denver Broncos',\n",
    "    'Detroit Lions',\n",
    "    'Green Bay Packers',\n",
    "    'Houston Texans',\n",
    "    'Indianapolis Colts',\n",
    "    'Jacksonville Jaguars',\n",
    "    'Kansas City Chiefs',\n",
    "    'Los Angeles Chargers',\n",
    "    'Miami Dolphins',\n",
    "    'Minnesota Vikings',\n",
    "    'New Orleans Saints',\n",
    "    'New York Giants',\n",
    "    'New York Jets',\n",
    "    'Philadelphia Eagles',\n",
    "    'San Francisco 49ers',\n",
    "    'Seattle Seahawks',\n",
    "    'Tampa Bay Buccaneers',\n",
    "    'Washington Commanders',]\n",
    "\n",
    "    return df_grouped[df_grouped['TEAM'].isin(teams)]\n",
    "\n",
    "def insert_espn_news():\n",
    "    logger.info(\"Starting espn_news updates\")\n",
    "    get_espn_news()\n",
    "    start_date, end_date = get_start_end()\n",
    "    df = get_unclassified(start_date, end_date)\n",
    "    df[\"relevant\"] = df[\"title\"].apply(check_relevance)\n",
    "    logger.info(f\"RAN check_relevance on {len(df)} rows\")\n",
    "    update_column('relevant', df)\n",
    "    #update_rows(df)\n",
    "    df = df[df['relevant']=='True']\n",
    "    df['article_text'] = df['link'].apply(extract_article_text)\n",
    "    df['ai_score'] = df['article_text'].apply(score_article)\n",
    "    logger.info(f\"RAN score_article on {len(df)} rows\")\n",
    "    update_column('ai_score', df)\n",
    "    logger.info(\"Completed espn_news updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "with sqlite3.connect('../data-log.db') as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query to get articles within the date range where relevant is None\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM espn_news\n",
    "    ''')\n",
    "# 1\n",
    "# Fetch all results\n",
    "data = cursor.fetchall()\n",
    "\n",
    "columns = ['title', 'date', 'link', 'image_url', 'relevant', 'ai_score']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n",
    "df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "\n",
    "final_results = []\n",
    "\n",
    "df = df[df['ai_score'].notna()]\n",
    "for ix, row in df.iterrows():\n",
    "    ai_score = json.loads(row['ai_score'].strip().replace('\\n', ''))\n",
    "    df_keys = list(ai_score.keys())\n",
    "    #df.at[ix, 'ai_score'] = \n",
    "    if df_keys:\n",
    "        if 'result' in ai_score or 'results' in ai_score:\n",
    "            for game in ai_score[df_keys[0]]:\n",
    "                final_results.append(game)\n",
    "        else:\n",
    "            final_results.append(row['ai_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Green Bay Packers': 2}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chicago Bears': -3}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(row['ai_score'].strip().replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "with sqlite3.connect('../data-log.db') as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query to get articles within the date range where relevant is None\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM espn_news\n",
    "    ''')\n",
    "# 1\n",
    "# Fetch all results\n",
    "data = cursor.fetchall()\n",
    "\n",
    "columns = ['title', 'date', 'link', 'image_url', 'relevant', 'ai_score']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n",
    "df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "\n",
    "final_results = []\n",
    "\n",
    "df = df[df['ai_score'].notna()]\n",
    "for ix, row in df.iterrows():\n",
    "    ai_score = json.loads(row['ai_score'].strip().replace('\\n', ''))\n",
    "    df_keys = list(ai_score.keys())\n",
    "    #df.at[ix, 'ai_score'] = \n",
    "    if df_keys:\n",
    "        if 'result' in ai_score or 'results' in ai_score:\n",
    "            for game in ai_score[df_keys[0]]:\n",
    "                final_results.append(game)\n",
    "        else:\n",
    "            final_results.append(row['ai_score'])\n",
    "\n",
    "def cast_int_or_zero(value):\n",
    "    try:\n",
    "        return(int(value))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df = pd.DataFrame(final_results)\n",
    "df[0] = df[0].apply(lambda x: x.replace('\\n', '').strip() if type(x) == str else x)\n",
    "# Melt the DataFrame to long format\n",
    "j_data = []\n",
    "for ix, row in df.iterrows():\n",
    "    try:\n",
    "        for team, value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n   \\n\\n\\n   \\n\\n\\n  \\n\\n    \\n\\n\\n\\n  \\n     \\n               \\n           \\n       \\n   \\n   \\n    \\n\\n\\n\\n   \\n  \\n\\n    \\n\\n    \\n\\n  \\n   \\n  \\n     \\n       \\n           \\n          \\n              \\n\\n   \\n    \\n\\n\\n    \\n\\n  \\n\\n \\n\\n   \\n\\n\\n\\n                  \\n            \\n       \\n       \\n         \\n   \\n    \\n\\n    \\n  \\n\\n    \\n\\n  \\n     \\n     \\n       \\n         \\n  \\n\\n   \\n\\n       \\n\\n       \\n              \\n            \\n            \\n                \\n                    \\n\\n    \\n              \\n\\n              \\n        \\n\\n      \\n           \\n\\n            \\n             \\n            \\n\\n          \\n     \\n\\n\\n    \\n\\n    \\n\\n \\n\\n\\n\\n\\n    \\n  \\n     \\n        \\n            \\n    \\n\\n\\n  \\n     \\n  \\n     \\n\\n      \\n  \\n   \\n      \\n     \\n\\n    \\n\\n    \\n\\n      \\n   \\n \\n  \\n      \\n     \\n        \\n\\n  \\n     \\n        \\n          \\n           \\n           \\n             \\n    \\n\\n    \\n\\n     \\n     \\n\\n        \\n           \\n\\n    \\n\\n  \\n     \\n             \\n         \\n\\n    \\n    \\n   \\n\\n\\n\\n  \\n   \\n     \\n\\n    \\n\\n      \\n   \\n    \\n  \\n\\n  \\n     \\n    \\n\\n  \\n      \\n    \\n\\n      \\n        \\n\\n   \\n       \\n\\n               \\n\\n             \\n    \\n  \\n     \\n  \\n\\n    \\n\\n    \\n\\n  \\n\\n   \\n\\n   \\n    \\n\\n    \\n\\n  \\n\\n    \\n\\n  \\n\\n   \\n\\n    \\n\\n               \\n      \\n    \\n                 \\n  \\n\\n    \\n\\n  \\n\\n     \\n  \\n\\n        \\n  \\n \\n\\n \\n   \\n\\n   \\n\\n   \\n\\n\\n  \\n\\n    \\n\\n     \\n\\n               \\n\\n      \\n   \\n   \\n\\n    \\n\\n     \\n         \\n\\n    \\n\\n  \\n\\n   \\n    \\n\\n  \\n\\n    \\n\\n  \\n      \\n   \\n\\n     \\n\\n    \\n    \\n\\n   \\n     \\n       \\n\\n  \\n  \\n\\n   \\n   \\n\\n     \\n   \\n\\n   \\n\\n  \\n \\n\\n      \\n   \\n\\n    \\n\\n    \\n  \\n    \\n     \\n      \\n  \\n   \\n   \\n\\n    \\n\\n\\n\\n  \\n    \\n    \\n\\n  \\n  \\n\\n    \\n\\n    \\n\\n     \\n     \\n      \\n    \\n     \\n\\n    \\n    \\n\\n    \\n    \\n \\n\\n \\n\\n\\n   \\n    \\n    \\n   \\n   \\n\\n    \\n  \\n     \\n\\n   \\n\\n    \\n\\n    \\n  \\n        \\n\\n \\n\\n  \\n       \\n            \\n\\n  \\n    \\n  \\n     \\n      \\n\\n      \\n    \\n\\n  \\n    \\n\\n         \\n     \\n            \\n\\n   \\n\\n  \\n  \\n\\n  \\n     \\n     \\n     \\n\\n   \\n    \\n      \\n\\n     \\n\\n     \\n\\n   \\n    \\n  \\n\\n  \\n    \\n    \\n\\n\\n    \\n\\n        \\n    \\n\\n\\n    \\n  \\n\\n   \\n    \\n    \\n\\n       \\n  \\n\\n\\n    \\n      \\n    \\n   \\n\\n    \\n\\n \\n\\n    \\n\\n    \\n  \\n    \\n\\n  \\n    \\n     \\n    \\n  \\n\\n    \\n\\n     \\n   \\n  \\n\\n  \\n    \\n  \\n   \\n    \\n  \\n  \\n\\n      \\n  \\n\\n   \\n\\n \\n   \\n\\n  \\n    \\n   \\n \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n   \\n    \\n\\n      \\n        \\n\\n   \\n\\n    \\n\\n  \\n\\n      \\n\\n       \\n\\n       \\n\\n   \\n    \\n\\n  \\n     \\n        \\n    \\n\\n    \\n\\n     \\n     \\n  \\n   \\n         \\n \\n\\n       \\n\\n\\n\\n   \\n    \\n\\n   \\n   \\n   \\n    \\n   \\n    \\n  \\n    \\n\\n    \\n    \\n     \\n\\n   \\n  \\n\\n  \\n   \\n   \\n  \\n\\n   \\n    \\n\\n  \\n    \\n\\n    \\n\\n   \\n  \\n    \\n     \\n \\n    \\n\\n    \\n      \\n  \\n    \\n  \\n   \\n    \\n    \\n\\n  \\n\\n\\n  \\n    \\n \\n    \\n\\n   \\n     \\n       \\n \\n\\n\\n\\n  \\n     \\n    \\n          \\n        \\n           \\n    \\n \\n  \\n    \\n  \\n    \\n  \\n    \\n   \\n  \\n  \\n   \\n     \\n      \\n   \\n    \\n                   \\n          \\n    \\n\\n  \\n\\n\\n  \\n \\n\\n \\n\\n    \\n        \\n           \\n\\n    \\n\\n   \\n\\n \\n   \\n\\n    \\n    \\n      \\n    \\n   \\n    \\n\\n      \\n        \\n    \\n\\n    \\n    \\n     \\n\\n      \\n    \\n\\n   \\n\\n     \\n      \\n  \\n   \\n\\n  \\n\\n    \\n\\n    \\n\\n     \\n\\n\\n    \\n\\n \\n        \\n   \\n\\n\\n   \\n  \\n\\n   \\n   \\n    \\n       \\n   \\n         \\n \\n    \\n          \\n    \\n  \\n\\n    \\n\\n   \\n\\n     \\n    \\n   \\n\\n\\n  \\n    \\n\\n   \\n\\n  \\n  \\n\\n    \\n  \\n\\n    \\n \\n    \\n     \\n\\n  \\n     \\n   \\n    \\n     \\n  \\n      \\n\\n  \\n    \\n\\n     \\n   \\n     \\n\\n  \\n   \\n\\n  \\n    \\n     \\n    \\n  \\n  \\n      \\n       \\n    \\n       \\n\\n  \\n    \\n    \\n\\n\\n    \\n\\n     \\n     \\n    \\n  \\n\\n\\n\\n     \\n\\n   \\n      \\n      \\n   \\n\\n    \\n   \\n\\n    \\n   \\n\\n    \\n  \\n    \\n    \\n    \\n\\n  \\n     \\n      \\n     \\n\\n\\n        \\n       \\n  \\n\\n\\n\\n     \\n\\n     \\n     \\n          \\n        \\n  \\n\\n            \\n      \\n\\n      \\n\\n        \\n       \\n     \\n     \\n\\n   \\n      \\n     \\n\\n\\n\\n \\n  \\n\\n  \\n\\n     \\n  \\n\\n     \\n    \\n\\n  \\n\\n\\n  \\n\\n      \\n     \\n    \\n  \\n        \\n\\n  \\n   \\n\\n    \\n   \\n  \\n   \\n\\n     \\n    \\n   \\n\\n   \\n \\n\\n    \\n \\n  \\n\\n     \\n    \\n \\n\\n \\n  \\n     \\n\\n    \\n     \\n  \\n  \\n\\n     \\n\\n\\n\\n\\n  \\n \\n  \\n  \\n\\n\\n    \\n  \\n\\n   \\n   \\n\\n\\n\\n  \\n     \\n        \\n          \\n    \\n     \\n      \\n  \\n    \\n  \\n       \\n\\n      \\n   \\n    \\n   \\n  \\n\\n  \\n    \\n  \\n   \\n  \\n     \\n    \\n        \\n\\n   \\n  \\n   \\n     \\n    \\n  \\n    \\n     \\n  \\n            \\n           \\n\\n        \\n        \\n  \\n     \\n     \\n\\n    \\n\\n    \\n\\n\\n\\n\\n \\n     \\n     \\n      \\n\\n  \\n\\n\\n   \\n    \\n        \\n\\n      \\n    \\n    \\n\\n        \\n  \\n    \\n  \\n    \\n  \\n     \\n\\n\\n      \\n\\n       \\n    \\n\\n    \\n  \\n  \\n        \\n    \\n    \\n      \\n  \\n\\n\\n   \\n         \\n        \\n      \\n    \\n\\n    \\n\\n    \\n    \\n  \\n  \\n\\n\\n\\n\\n    \\n\\n   \\n\\n    \\n\\n        \\n    \\n   \\n    \\n\\n    \\n  \\n        \\n     \\n  \\n\\n    \\n\\n     \\n    \\n  \\n      \\n  \\n\\n     \\n\\n    \\n\\n    \\n\\n     \\n   \\n     \\n\\n  \\n    \\n\\n        \\n    \\n  \\n\\n      \\n  \\n        \\n    \\n       \\n\\n\\n   \\n\\n      \\n     \\n\\n     \\n\\n     \\n     \\n               \\n     \\n      \\n\\n\\n   \\n\\n        \\n          \\n              \\n    \\n    \\n  \\n  \\n\\n    \\n\\n    \\n       \\n    \\n\\n\\n    \\n        \\n     \\n                \\n  \\n    \\n\\n     \\n\\n     \\n\\n  \\n\\n\\n          \\n    \\n\\n     \\n\\n    \\n  \\n\\n    \\n     \\n\\n      \\n   \\n\\n\\n    \\n\\n  \\n        \\n   \\n  \\n \\n    \\n     \\n    \\n    \\n\\n    \\n     \\n \\n      \\n  \\n    \\n\\n  \\n   \\n   \\n    \\n     \\n\\n    \\n      \\n   \\n\\n    \\n\\n     \\n     \\n     \\n\\n\\n    \\n     \\n    \\n    \\n\\n\\n      \\n\\n     \\n\\n  \\n\\n  \\n\\n    \\n\\n       \\n\\n    \\n  \\n\\n    \\n    \\n     \\n        \\n         \\n   \\n      \\n  \\n    \\n    \\n      \\n\\n    \\n  \\n    \\n     \\n    \\n    \\n        \\n        \\n  \\n\\n    \\n  \\n\\n     \\n    \\n       \\n\\n    \\n\\n      \\n    \\n    \\n  \\n    \\n\\n    \\n     \\n\\n      \\n   \\n    \\n    \\n\\n  \\n      \\n    \\n    \\n\\n     \\n        \\n       \\n     \\n    \\n  \\n\\n       \\n\\n\\n  \\n\\n   \\n\\n     \\n        \\n\\n\\n    \\n   \\n     \\n      \\n    \\n\\n\\n       \\n  \\n    \\n  \\n         \\n   \\n     \\n    \\n    \\n    \\n    \\n    \\n\\n \\n   \\n\\n    \\n    \\n\\n        \\n      \\n    \\n \\n    \\n\\n     \\n        \\n\\n  \\n\\n    \\n    \\n\\n          \\n    \\n    \\n   \\n    \\n     \\n        \\n  \\n   \\n     \\n            \\n        \\n        \\n        \\n         \\n        \\n  \\n    \\n\\n      \\n     \\n  \\n\\n   \\n    \\n    \\n\\n\\n  \\n  \\n\\n    \\n    \\n\\n  \\n   \\n\\n  \\n     \\n    \\n     \\n\\n       \\n    \\n    \\n    \\n  \\n    \\n    \\n     \\n    \\n        \\n   \\n\\n    \\n\\n    \\n        \\n\\n           \\n            \\n      \\n\\n      \\n   \\n\\n\\n  \\n     \\n     \\n\\n \\n\\n\\n  \\n       \\n\\n   \\n   \\n    \\n\\n    \\n     \\n \\n   \\n       \\n  \\n    \\n   \\n    \\n     \\n\\n     \\n\\n    \\n\\n  \\n\\n      \\n   \\n    \\n    \\n\\n      \\n    \\n     \\n  \\n\\n\\n\\n   \\n\\n    \\n\\n    \\n\\n    \\n\\n      \\n    \\n      \\n            \\n\\n\\n  \\n   \\n\\n      \\n   \\n\\n    \\n\\n     \\n    \\n     \\n   \\n   \\n     \\n    \\n\\n    \\n\\n  \\n\\n       \\n  \\n    \\n                 \\n   \\n\\n   \\n     \\n   \\n    \\n\\n    \\n       \\n    \\n    \\n    \\n    \\n   \\n     \\n      \\n      \\n       \\n  \\n\\n\\n   \\n   \\n        \\n\\n     \\n    \\n\\n   \\n     \\n    \\n        \\n  \\n    \\n    \\n    \\n    \\n    \\n     \\n        \\n    \\n\\n      \\n    \\n\\n  \\n     \\n      \\n \\n    \\n\\n  \\n\\n         \\n  \\n     \\n  \\n\\n   \\n    \\n    \\n    \\n    \\n       \\n  \\n    \\n    \\n\\n\\n\\n   \\n    \\n     \\n\\n  \\n     \\n   \\n     \\n        \\n    \\n      \\n       \\n    \\n   \\n\\n      \\n    \\n    \\n     \\n        \\n      \\n   \\n\\n     \\n   \\n    \\n \\n    \\n\\n  \\n   \\n    \\n\\n\\n     \\n    \\n\\n   \\n    \\n     \\n     \\n    \\n    \\n   \\n     \\n\\n    \\n\\n    \\n\\n      \\n        \\n     \\n    \\n   \\n    \\n     \\n    \\n     \\n \\n   \\n   \\n  \\n\\n    \\n\\n \\n    \\n   \\n      \\n    \\n\\n\\n\\n     \\n  \\n\\n   \\n\\n      \\n\\n    \\n     \\n    \\n   \\n\\n     \\n      \\n\\n    \\n   \\n        \\n      \\n   \\n        \\n \\n   \\n\\n      \\n  \\n\\n    \\n\\n\\n\\n        \\n      \\n      \\n   \\n \\n   \\n\\n     \\n     \\n       \\n   \\n \\n   \\n   \\n        \\n     \\n\\n     \\n\\n    \\n\\n    \\n\\n   \\n    \\n    \\n    \\n    \\n  \\n\\n      \\n\\n    \\n   \\n    \\n    \\n   \\n    \\n   \\n    \\n\\n\\n  \\n    \\n   \\n    \\n   \\n    \\n    \\n     \\n    \\n\\n     \\n       \\n\\n   \\n    \\n  \\n\\n   \\n\\n     \\n     \\n\\n    \\n\\n    \\n  \\n      \\n    \\n  \\n  \\n\\n  \\n    \\n     \\n\\n    \\n\\n     \\n    \\n\\n\\n    \\n  \\n    \\n    \\n         \\n    \\n\\n    \\n       \\n\\n    \\n   \\n\\n   \\n  \\n    \\n    \\n   \\n\\n  \\n    \\n\\n    \\n\\n     \\n      \\n      \\n      \\n      \\n \\n\\n\\n    \\n    \\n\\n  \\n    \\n     \\n     \\n   \\n\\n\\n    \\n\\n       \\n\\n    \\n \\n \\n \\n\\n\\n    \\n\\n  \\n\\n    \\n        \\n   \\n  \\n    \\n     \\n     \\n    \\n\\n     \\n \\n  \\n \\n \\n     \\n     \\n\\n\\n    \\n    \\n   \\n    \\n    \\n     \\n    \\n      \\n\\n\\n        \\n    \\n\\n     \\n\\n\\n  \\n    \\n        \\n\\n   \\n\\n    \\n\\n    \\n    \\n   \\n    \\n  \\n\\n    \\n       \\n    \\n  \\n\\n   \\n\\n    \\n\\n      \\n\\n    \\n        \\n  \\n       \\n\\n    \\n  \\n   \\n   \\n         \\n     \\n        \\n   \\n        \\n    \\n    \\n\\n  \\n      \\n   \\n    \\n    \\n\\n  \\n        \\n    \\n    \\n\\n      \\n  \\n    \\n   \\n   \\n     \\n\\n    \\n\\n   \\n  \\n\\n       \\n  \\n    \\n     \\n     \\n     \\n     \\n\\n\\n    \\n   \\n\\n  \\n   \\n    \\n    \\n\\n   \\n\\n  \\n        \\n     \\n     \\n \\n\\n     \\n\\n  \\n    \\n    \\n     \\n  \\n    \\n  \\n      \\n     \\n  \\n  \\n\\n   \\n  \\n    \\n\\n   \\n  \\n\\n\\n    \\n\\n    \\n\\n    \\n\\n     \\n\\n  \\n      \\n     \\n    \\n    \\n\\n    \\n  \\n       \\n     \\n   \\n     \\n  \\n    \\n     \\n    \\n  \\n\\n    \\n\\n  \\n   \\n\\n    \\n     \\n\\n    \\n\\n    \\n    \\n  \\n\\n    \\n     \\n       \\n\\n    \\n    \\n\\n     \\n\\n  \\n    \\n    \\n     \\n \\n\\n       \\n    \\n\\n    \\n  \\n     \\n    \\n     \\n\\n     \\n    \\n     \\n   \\n\\n    \\n\\n   \\n    \\n    \\n  \\n       \\n    \\n     \\n\\n   \\n    \\n\\n\\n  \\n    \\n\\n   \\n\\n      \\n\\n    \\n     \\n    \\n    \\n      \\n \\n    \\n\\n    \\n     \\n  \\n\\n\\n     \\n\\n      \\n        \\n\\n     \\n\\n\\n   \\n    \\n  \\n  {\\n  \"Chicago Bears\": -3\\n}\\n',\n",
       " '\\n{\"Jacksonville Jaguars\": -2}',\n",
       " {'T.J. Hockenson': 1},\n",
       " {'Los Angeles Chargers': 3},\n",
       " {'Houston Texans': 4},\n",
       " {'Philadelphia Eagles': 0},\n",
       " {'Baltimore Ravens': 0},\n",
       " {'New Orleans Saints': 2},\n",
       " '{\"New York Giants\": -5}',\n",
       " '{\"Dallas Cowboys\": 2}',\n",
       " '{\\n    \"team\": {\\n        \"Chicago Bears\": -3\\n    }\\n  }',\n",
       " '\\n{ \"Miami Dolphins\": -1 }',\n",
       " '\\n{\"New York Giants\": -4}',\n",
       " '\\n \\t{\"Kansas City Chiefs\": 4}',\n",
       " '\\n{\"Detroit Lions\": 4}',\n",
       " '{\"Tampa Bay Buccaneers\": 3}',\n",
       " '\\n{\"San Francisco 49ers\": -2}',\n",
       " '{\"Washington Commanders\": -2}',\n",
       " '\\n{\"Detroit Lions\": 3}',\n",
       " {'Detroit Lions': 4},\n",
       " {'Dallas Cowboys': 3},\n",
       " {'Miami Dolphins': 0},\n",
       " {'Kansas City Chiefs': 4},\n",
       " {'Los Angeles Chargers': 1},\n",
       " {'Cincinnati Bengals': 2},\n",
       " {'Jacksonville Jaguars': 1},\n",
       " {'Minnesota Vikings': 4},\n",
       " {'Indianapolis Colts': 2},\n",
       " {'Seattle Seahawks': 1},\n",
       " {'Washington Commanders': 1},\n",
       " {'Tampa Bay Buccaneers': 2},\n",
       " {'New Orleans Saints': 2},\n",
       " {'Philadelphia Eagles': 3},\n",
       " {'Buffalo Bills': 4},\n",
       " {'Denver Broncos': 3},\n",
       " {'Kansas City Chiefs': 4},\n",
       " {'Los Angeles Chargers': 3},\n",
       " {'Cincinnati Bengals': 2},\n",
       " {'Houston Texans': -1},\n",
       " {'Arizona Cardinals': 2},\n",
       " {'Indianapolis Colts': 2},\n",
       " {'New York Jets': 1},\n",
       " {'Washington Commanders': 0},\n",
       " {'Tampa Bay Buccaneers': 3},\n",
       " {'New Orleans Saints': 3},\n",
       " {'Baltimore Ravens': 2},\n",
       " {'Buffalo Bills': 4},\n",
       " {'Denver Broncos': 3},\n",
       " {'Detroit Lions': 5},\n",
       " {'Dallas Cowboys': 2},\n",
       " {'Green Bay Packers': 2},\n",
       " '\\n{ \"team_name\": \"Washington Commanders\", \"rating\": 3 }\\n',\n",
       " '\\n     {\\n        \"Houston Texans\": -2\\n    }\\n    \\n\\n\\n     ',\n",
       " '{\"None\": 0}',\n",
       " '\\n  {\"Kansas City Chiefs\": 5}',\n",
       " '\\n{\"San Francisco 49ers\": -1}',\n",
       " '\\n{ \"Team\": \"None\", \"Score\": 0 }',\n",
       " '\\n\\n{ \"Minnesota Vikings\": 4 }']"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the Score column\n",
    "df_melted['value'] = df_melted['value'].apply(lambda x: cast_int_or_zero(x))\n",
    "df_melted = df_melted.dropna(subset=['value'])\n",
    "\n",
    "# Group by TEAM and calculate AVG and SUM\n",
    "df_grouped = df_melted.groupby('TEAM')['value'].agg(['mean', 'sum']).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "df_grouped.columns = ['TEAM', 'AVG', 'SUM']\n",
    "\n",
    "teams = ['Arizona Cardinals',\n",
    "'Baltimore Ravens',\n",
    "'Buffalo Bills',\n",
    "'Chicago Bears',\n",
    "'Cincinnati Bengals',\n",
    "'Dallas Cowboys',\n",
    "'Denver Broncos',\n",
    "'Detroit Lions',\n",
    "'Green Bay Packers',\n",
    "'Houston Texans',\n",
    "'Indianapolis Colts',\n",
    "'Jacksonville Jaguars',\n",
    "'Kansas City Chiefs',\n",
    "'Los Angeles Chargers',\n",
    "'Miami Dolphins',\n",
    "'Minnesota Vikings',\n",
    "'New Orleans Saints',\n",
    "'New York Giants',\n",
    "'New York Jets',\n",
    "'Philadelphia Eagles',\n",
    "'San Francisco 49ers',\n",
    "'Seattle Seahawks',\n",
    "'Tampa Bay Buccaneers',\n",
    "'Washington Commanders',]\n",
    "\n",
    "df_grouped[df_grouped['TEAM'].isin(teams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TEAM, value]\n",
       "Index: []"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl-docker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
